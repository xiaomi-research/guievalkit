import os

from pydantic import BaseModel, Field, computed_field, model_validator
from typing import Literal
from typing_extensions import Self

# subsec internal
from vllm_serve import ModelSetup

DATASET = Literal["androidcontrol_low",
                  "androidcontrol_high",
                  "cagui_agent",
                  "gui_odyssey"]


class EvalTaskConfig(BaseModel):
    model: ModelSetup = Field(
        description="The model setup for the evaluation.",
        frozen=True)

    datasets: str | DATASET | list[str | DATASET] = Field(
        default=['cagui_agent', ],
        description=("The datasets to evaluate on. "
                     "Provide one or more values; comma-separated string is also accepted."),
        frozen=True, validate_default=True)

    eval_mode: Literal["offline_rule", "offline_model",
                       "semi_online", "semi_online_concise", "semi_online_model"] = Field(
        default="offline_rule",
        description=("Specify the evaluation mode.\n"
                     "`offline_rule`: evaluate with model off-policy based on content generated by predefined rules.\n"
                     "`offline_model`: evaluate with model off-policy based on content generated by other models.\n"
                     "`semi_online`: evaluate with model on-policy based on content generated by the model itself "
                     "if the corresponding task succeeds.\n"
                     "\tand that generated by predefined rules if the corresponding task fails.\n"
                     "`semi_online_concise`: "
                     "evaluate with model on-policy based on content generated by the model itself "
                     "if the corresponding task succeeds without thinking.\n"
                     "\tand that generated by predefined rules if the corresponding task fails.\n"
                     "`semi_online_model`: "
                     "evaluate with model on-policy based on content generated by the model itself "
                     "if the corresponding task succeeds.\n"
                     "\tand that generated by other models if the corresponding task fails."),
        frozen=True, validate_default=True)

    enable_thinking: bool = Field(default=True,
                                  description="Whether to enable thinking for the model.",
                                  frozen=True, validate_default=True)
    enable_conclusion: bool = Field(default=True,
                                   description="Whether to enable conclusion for the model.",
                                   frozen=True, validate_default=True)

    output_dir: str = Field(default="./outputs",
                            description="The directory to save the evaluation results.",
                            frozen=True, validate_default=True)
    log_dir: str = Field(default='./logs/guieval',
                         description='The root of the destination saving the runtime log.',
                         frozen=True, validate_default=True)

    vllm_mode: Literal["offline", "online", False] = Field(
        default="online",
        description=("Whether to use vLLM for the tasks and the inference mode.\n"
                     "\t`offline`: use vLLM.LLM.generate for batched generation\n"
                     "\t`online`: use vLLM online serving for concurrent generation\n"
                     "\t`False`: use transformers for generation. Currently not supported."),
        frozen=True, validate_default=True)

    batch_size: int = Field(default=64,
                            description="The batch size for the tasks when using vllm_mode='offline'.",
                            frozen=True, validate_default=True,
                            ge=1)
    max_concurrent_tasks: int = Field(default=128,
                                      description=("The maximum number of concurrent tasks "
                                                   "when using vllm_mode='online'."),
                                      frozen=True, validate_default=True,
                                      ge=1)

    @computed_field
    @property
    def model_output_dir(self) -> str:
        return os.path.join(self.output_dir, self.model.model_alias,
                            self.eval_mode)  # memo thinking ~

    @computed_field
    @property
    def predictions_output_dir(self) -> str:
        thinking_tag = '_thinking' if self.enable_thinking else ''
        return os.path.join(self.model_output_dir, f"predictions{thinking_tag}")

    @computed_field
    @property
    def evaluation_file(self) -> str:
        return os.path.join(self.model_output_dir,
                            f"evaluation_thinking_{self.enable_thinking}.json")

    @computed_field
    @property
    def log_output_dir(self) -> str:
        return os.path.join(self.log_dir, f'{self.model.model_alias}')

    @computed_field
    @property
    def log_file(self) -> str:
        return os.path.join(self.log_output_dir, f'{self.model.timestamp}.log')

    @model_validator(mode='before')
    @classmethod
    def _parse_datasets(cls, data: dict) -> dict:
        datasets = data.get('datasets')
        if isinstance(datasets, str):
            data['datasets'] = [s.strip() for s in datasets.split(',') if s.strip()]

        return data

    @model_validator(mode='before')
    @classmethod
    def _validate_concurent_workers(cls, data: dict) -> dict:
        if data['vllm_mode'] == 'offline':
            data["max_concurrent_tasks"] = 1  # the offline inference api of vLLM is not concurrent
        return data

    @model_validator(mode='before')
    @classmethod
    def _validate_batch_size(cls, data: dict) -> dict:
        if data['vllm_mode'] == 'online':
            data["batch_size"] = 1  # when using concurrent generation, no need to batch the requests
        return data

    @model_validator(mode='before')
    @classmethod
    def _validate_vllm_mode(cls, data: dict) -> dict:
        if not data['vllm_mode']:
            raise NotImplementedError("Using transformers for generation is currently not supported.")
        return data

    @model_validator(mode='after')
    def _validate_output_dir(self) -> Self:
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)
        if not os.path.exists(self.model_output_dir):
            os.makedirs(self.model_output_dir)
        if not os.path.exists(self.predictions_output_dir):
            os.makedirs(self.predictions_output_dir)
        if not os.path.exists(self.log_output_dir):
            os.makedirs(self.log_output_dir)
        return self
