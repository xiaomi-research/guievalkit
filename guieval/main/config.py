import os
from pydantic import BaseModel, Field, computed_field, model_validator
from typing import Literal
from typing_extensions import Self

# subsec internal
from vllm_serve import ModelSetup
from guieval.main.history_sampler.abcsampler import HistorySamplerConfig
from guieval.main.utils import DATASET


class EvalTaskConfig(BaseModel):
    model: ModelSetup = Field(
        description="The model setup for the evaluation.",
        frozen=True)

    datasets: str | DATASET | list[str | DATASET] = Field(
        default=['cagui_agent', ],
        description=("The datasets to evaluate on. "
                     "Provide one or more values; comma-separated string is also accepted."),
        frozen=True, validate_default=True)

    eval_mode: Literal["offline_rule", "offline_model",
                       "semi_online", "semi_online_concise", "semi_online_model"] = Field(
                           default="offline_rule",
                           description=("Specify the evaluation mode.\n"
                                        "`offline_rule`: evaluate with model off-policy based on content "
                                        "generated by predefined rules.\n"
                                        "`offline_model`: evaluate with model off-policy based on "
                                        "content generated by other models.\n"
                                        "`semi_online`: evaluate with model on-policy based on "
                                        "content generated by the model itself if the corresponding task succeeds.\n"
                                        "\tand that generated by predefined rules if the corresponding task fails.\n"
                                        "`semi_online_concise`: evaluate with model on-policy "
                                        "based on content generated by the model itself if the corresponding task "
                                        "succeeds without thinking.\n"
                                        "\tand that generated by predefined rules if the corresponding task fails.\n"
                                        "`semi_online_model`: evaluate with model on-policy based "
                                        "on content generated by the model itself if the corresponding task succeeds.\n"
                                        "\tand that generated by other models if the corresponding task fails."),
                           frozen=True, validate_default=True)

    enable_thinking: bool = Field(default=True,
                                  description="Whether to enable thinking for the model.",
                                  frozen=True, validate_default=True)
    enable_conclusion: bool = Field(default=True,
                                   description="Whether to enable conclusion for the model.",
                                   frozen=True, validate_default=True)

    output_dir: str = Field(default="./outputs",
                            description="The directory to save the evaluation results.",
                            frozen=True, validate_default=True)
    log_dir: str = Field(default='./logs/guieval',
                         description='The root of the destination saving the runtime log.',
                         frozen=True, validate_default=True)
    restart: bool = Field(default=False,
                          description=("If True, this run would read from the existing output directory and"
                                       " continue the evaluation with the existing results.\n"
                                       "Otherwise, all the existing results would be overwritten."),
                          frozen=False, validate_default=True)

    fix_memory: str | None = Field(default=None,
                                   description=("If passed valid path with saved predictions, the memory unit "
                                                "would be prefilled for semi-online evaluation, "
                                                "instead of dynamically filling with the generated results during "
                                                "runtime."
                                                "Be aware that it would also block the memory "
                                                "prefilling of existing task results for restart evaluation."),
                                   frozen=True, validate_default=True)
    fix_thought: str | None = Field(default=None,
                                    description=("If passed valid path with saved thoughts, the memory "
                                                 "unit would be prefilled for semi-online evaluation, "
                                                 "instead of dynamically filling "
                                                 "with the generated memories during runtime."
                                                 "Be aware that it would also block the memory "
                                                 "prefilling of existing task results for restart evaluation."),
                                    frozen=True, validate_default=True)
    fixed_thought_chat_template: str | None = Field(default=None,
                                                    description=("It would be used to format the messages "
                                                                 "with the fixed thought."),
                                                    frozen=True, validate_default=True)

    history_sampler: HistorySamplerConfig = Field(
        default_factory=HistorySamplerConfig,
        validate_default=True,
        description="The history_content_sampler setup",
        frozen=True)

    vllm_mode: Literal["offline", "online", False] = Field(default="online",
                                                           description=("Whether to use vLLM for the tasks and "
                                                                        "the inference mode.\n"
                                                                        "\t`offline`: use vLLM.LLM.generate for "
                                                                        "batched generation\n"
                                                                        "\t`online`: use vLLM online serving for "
                                                                        "concurrent generation\n"
                                                                        "\t`False`: use transformers for generation. "
                                                                        "Currently not supported."),
                                                           frozen=True, validate_default=True)
    batch_size: int = Field(default=64,
                            description="The batch size for the tasks when using vllm_mode='offline'.",
                            frozen=True, validate_default=True,
                            ge=1)
    max_concurrent_tasks: int = Field(default=128,
                                      description="The maximum number of concurrent "
                                      "tasks when using vllm_mode='online'.",
                                      frozen=True, validate_default=True,
                                      ge=1)

    sample_size: int | None = Field(default=None,
                                    description=("The sample size for each step task during inference."
                                                 "It would override the vllm.SamplingParams.n value. "
                                                 "It can be taken as one way for test-time performance evaluation."),
                                    frozen=True, validate_default=True,
                                    ge=1)
    sample_seed: int | None = Field(default=None,
                                    description=("The seed for the inference sampling. "
                                                 "It would override the vllm.SamplingParams.seed value. "
                                                 "It would ensure the reproducibility of the inference sampling."),
                                    frozen=True, validate_default=True,
                                    ge=0)
    temperature: float | None = Field(default=None,
                                      description=("The temperature for the inference sampling. "
                                                   "It would override the vllm.SamplingParams.temperature value. "
                                                   "It would increase the diversity of the inference sampling."),
                                      frozen=True, validate_default=True,
                                      ge=0)
    top_p: float | None = Field(default=None,
                                description=("The top-p for the inference sampling. "
                                             "It would override the vllm.SamplingParams.top_p value. "
                                             "It would increase the diversity of the inference sampling."),
                                frozen=True, validate_default=True,
                                gt=0, le=1)
    top_k: int | None = Field(default=None,
                              description=("The top-k for the inference sampling. "
                                           "It would override the vllm.SamplingParams.top_k value. "
                                           "It would increase the diversity of the inference sampling."),
                              frozen=True, validate_default=True,
                              ge=0)
    repetition_penalty: float | None = Field(default=None,
                                            description=("The repetition penalty for the inference sampling. "
                                                         "It would override the "
                                                         "vllm.SamplingParams.repetition_penalty. "
                                                         "It would penalize the repetition "
                                                         "of the tokens in the inference sampling."),
                                            frozen=True, validate_default=True,
                                            ge=0)
    presence_penalty: float | None = Field(default=None,
                                           description=("The presence penalty for the inference sampling. "
                                                        "It would override the vllm.SamplingParams.presence_penalty. "
                                                        "It would penalize the presence of the "
                                                        "tokens in the inference sampling."),
                                           frozen=True, validate_default=True,
                                           ge=0)
    max_tokens: int | None = Field(default=None,
                                  description=("The maximum number of tokens for the inference sampling. "
                                               "It would override the vllm.SamplingParams.max_tokens value. "
                                               "It would limit the length of the inference sampling."),
                                  frozen=True, validate_default=True,
                                  gt=0)

    @computed_field
    @property
    def model_output_dir(self) -> str:
        return os.path.join(self.output_dir, self.model.model_alias,
                            self.eval_mode)  # memo thinking ~

    @computed_field
    @property
    def prediction_output_dir(self) -> str:
        return os.path.join(self.model_output_dir, f"predictions_thinking_{self.enable_thinking}")

    @computed_field
    @property
    def prediction_output_paths(self) -> list[str]:
        return [os.path.join(self.prediction_output_dir, f"{dataset}.jsonl")
                for dataset in self.datasets]

    @computed_field
    @property
    def evaluation_file(self) -> str:
        return os.path.join(self.model_output_dir,
                            f"evaluation_thinking_{self.enable_thinking}.json")

    @computed_field
    @property
    def log_output_dir(self) -> str:
        return os.path.join(self.log_dir, f'{self.model.model_alias}')

    @computed_field
    @property
    def log_file(self) -> str:
        return os.path.join(self.log_output_dir, f'{self.model.timestamp}.log')

    @model_validator(mode='before')
    @classmethod
    def _parse_datasets(cls, data: dict) -> dict:
        datasets = data.get('datasets')
        if isinstance(datasets, str):
            data['datasets'] = [s.strip() for s in datasets.split(',') if s.strip()]

        return data

    @model_validator(mode='before')
    @classmethod
    def _validate_concurent_workers(cls, data: dict) -> dict:
        if data['vllm_mode'] == 'offline':
            data["max_concurrent_tasks"] = 1  # the offline inference api of vLLM is not concurrent
        return data

    @model_validator(mode='before')
    @classmethod
    def _validate_batch_size(cls, data: dict) -> dict:
        if data['vllm_mode'] == 'online':
            data["batch_size"] = 1  # when using concurrent generation, no need to batch the requests
        return data

    @model_validator(mode='before')
    @classmethod
    def _validate_vllm_mode(cls, data: dict) -> dict:
        if not data['vllm_mode']:
            raise NotImplementedError("Using transformers for generation is currently not supported.")
        return data

    @model_validator(mode='before')
    @classmethod
    def _validate_fix_memory(cls, data: dict) -> dict:
        if data['fix_memory'] is not None and not os.path.exists(data['fix_memory']):
            raise FileNotFoundError(f"The path {data['fix_memory']} for prefilled memory does not exist.")
        return data

    @model_validator(mode='after')
    def _validate_output_dir(self) -> Self:
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)
        if not os.path.exists(self.model_output_dir):
            os.makedirs(self.model_output_dir)
        if not os.path.exists(self.prediction_output_dir):
            os.makedirs(self.prediction_output_dir)
        if not os.path.exists(self.log_output_dir):
            os.makedirs(self.log_output_dir)
        return self

    @model_validator(mode='after')
    def _validate_fix_thought(self) -> Self:
        if self.fix_thought is not None:
            if self.fixed_thought_chat_template is None:
                raise ValueError("The fixed_thought_chat_template is required when you try to fix the thought.")
            if not os.path.exists(self.fix_thought):
                raise FileNotFoundError(f"The path {self.fix_thought} for prefilled thought does not exist.")
            if not os.path.exists(self.fixed_thought_chat_template):
                raise FileNotFoundError(f"The path {self.fixed_thought_chat_template} "
                                        "for the template of messages with "
                                        "the fixed thought does not exist.")

            self.model.chat_template = self.fixed_thought_chat_template  # update the chat template for the model
        return self
